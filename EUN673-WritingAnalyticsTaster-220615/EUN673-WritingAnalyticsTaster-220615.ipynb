{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Writing Analytics Taster\n",
    "\n",
    "*[Andrew Gibson](http://staff.qut.edu.au/staff/andrew.gibson) - EUN673 Presentation - 15 June 2022*\n",
    "\n",
    "The purpose of this notebook is to provide you with a bit of a taster of *Writing Analytics* as a sub-field of *Learning Analytics*, and in particular highlight some of the key approaches that are taken and some of the significant challenges that are encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Check out the Handbook\n",
    "\n",
    "[The Handbook of Learning Analytics - Online](https://www.solaresearch.org/publications/hla-22/) \n",
    "\n",
    "**PDF Downloads:**\n",
    "\n",
    "[The Handbook of Learning Analytics.](https://solaresearch.org/wp-content/uploads/hla22/HLA22.pdf) 2nd Ed. Charles Lang, George Siemens, Alyssa Friend Wise, Dragan Gaševic, and Agathe Merceron. (Eds.) Vancouver, Canada: SoLAR.\n",
    "\n",
    "\n",
    "Gibson, A., & Shibani, A. (2022). [Natural Language Processing-Writing Analytics.](https://solaresearch.org/wp-content/uploads/hla22/HLA22_Chapter_10_Gibson.pdf) In Charles Lang, George Siemens, Alyssa Friend Wise, Dragan Gaševic, and Agathe Merceron (Eds.) *The Handbook of Learning Analytics.* 2nd Ed. Vancouver, Canada: SoLAR, 96-104."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How do we approach WA?\n",
    "\n",
    "- Linguistic orientation\n",
    "- Domain orientation\n",
    "\n",
    "#### What is the intention for using WA?\n",
    "\n",
    "- Descriptive WA\n",
    "- Evaluative WA\n",
    "\n",
    "#### How do our orientations and intentions influence the efficacy of the WA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## >> Three *opinionated* principles..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1 - WA *should be* pragmatic\n",
    "\n",
    "Gibson, Andrew & Lang, Charles. (2018) [The pragmatic maxim as learning analytics research method. ](https://eprints.qut.edu.au/118262/22/118262.pdf) In Ochoa, X, Ferguson, R, Merceron, A, & Buckingham Shum, S (Eds.) Proceedings of the 8th International Conference on Learning Analytics and Knowledge.\n",
    "Association for Computing Machinery, United States of America, pp. 461- 465.\n",
    "\n",
    "> Consider what effects, that might conceivably have practical bearings, we conceive the object of our con- ception to have. Then, our conception of these effects is the whole of our conception of the object. (Charles Sanders Peirce, 1878, How to Make Our Ideas Clear)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pragmatic Inquiry for Learning Analytics Research (PILAR)\n",
    "\n",
    "1. **Contextualise** irritation or doubt within a clearly designed learning situation\n",
    "2. **Clarify** the analytics to investigate based on practical learning effects\n",
    "3. **Hypothesise** how the analytics will result in the practical effects\n",
    "4. **Apply** the hypothesis by putting it to the test in the learning context\n",
    "5. **Evaluate** the extent to which the hypothesis is true and practical effects are realised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2 - WA *should be* socio-technical\n",
    "\n",
    "Knight, S., Gibson, A., & Shibani, A. (2020). [Implementing Learning Analytics for Learning Impact: Taking Tools to Task.](https://eprints.qut.edu.au/197662/1/Social_and_Technical_Infrastructure_ORO.pdf) Internet and Higher Education. https://doi.org/10.1016/j.iheduc.2020.100729\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/andrewresearch/presentations/blob/79d0e1a4cc448b7a6db8132d2255aaa0b80735f2/EUN673-WritingAnalyticsTaster-220615/WAlayers.png?raw=true\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3 - WA *should* transcend accurate measurement\n",
    "\n",
    "Kitto, K., Buckingham Shum, S., & Gibson, A. (2018). [Embracing imperfection in learning analytics.](https://opus.lib.uts.edu.au/bitstream/10453/133000/1/p451-kitto.pdf) In Proceedings of the 8th international conference on learning analytics and knowledge (pp. 451-460).\n",
    "\n",
    "- What is the learning activity in which this WA should be used?\n",
    "- What model is used to link low level data to the WA?\n",
    "- What form of feedback is provided to learners?\n",
    "- In what way does the feedback contribute to learning gain?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## >> Exploring implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 1: Towards the discovery of metacognition from reflective writing\n",
    "\n",
    "Gibson, A., Kirsty, K., and Bruza, P. (2016) Towards the Discovery of Learner Metacognition From Reflective Writing. Journal of Learning Analytics, 3(2), 22-36. doi: [http://dx.doi.org/10.18608/jla.2016.32.3](http://dx.doi.org/10.18608/jla.2016.32.3)\n",
    "\n",
    "<div>\n",
    "<img src=\"http://nlytx.io/metacognition/assets/images/ReflectionMetacognitionSpectrum.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "[Experiment by clicking on the live demo: http://nlytx.io/metacognition/](http://nlytx.io/metacognition/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AhFOUj6iCWG",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2: Computer features ≠ Human concepts\n",
    "\n",
    "*Derived from LASI2022 Writing Analytics Tutorial*\n",
    "\n",
    "We can use Natural Language Processing (NLP) to perform low-level analysis of text. However, this is not particularly useful to most people (except perhaps linguists). To make the analysis useful for feedback on writing, we need to transform computational analysis into human understandable features that have a clear meaning for the writer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate, let's look at a short demo reflection I wrote for one of my data analytics classes..\n",
    "\n",
    "> Yesterday I went to a seminar on deep learning that really challenged me to consider whether I am taking full advantage of this new technology in my research. Jeremy and Rachel presented about why deep learning is not over-hyped, and how it is going to be significant in the longer term. Initially, I was underwhelmed by this idea, and was thinking that the seminar might not hold much value for me. However, some of the material they presented was surprising and challenged me to re-think where I was at. Jeremy suggested that deep learning technologies were similar to electricity and that often the immediate applications are not obvious, but as time goes on, the technology is seen to be more valuable in many areas of society. He suggested that this could also be the case for deep learning technologies. I certainly hadn’t thought much about this with respect to my own research. However, this idea suggested that perhaps I need to reconsider whether it may be a valuable tech to add to my current technologies that I use.\n",
    " I have tended to avoid using the technology in my own research, for many reasons, and Jeremy and Rachel highlighted some of these reasons as common reasons why people avoid the technology. They then presented counter examples to challenge that thinking. This certainly challenged me in some ways, and I at times I felt a bit uncomfortable in seeing how flimsy some of my reasoning was. \n",
    "Certainly, moving forward I should probably be more open to how some of these new developments might be able to aid the advancement of my work in reflective writing analytics. I think that although there is still good reason to be careful, and aware of poor uses of this technology, I have tended to use this as bit of an excuse to avoid taking the extra effort to try and apply the technology in my own work.\n",
    "Moving forward, I need to take some time to look carefully at which applications might advance my research, and give them a go. Although I need to balance the use of deep learning with my other techniques, I should be careful not to favour those techniques that have served me well in the past, and be blinded to the potential of new (and potentially better) techniques. I should at least be open to the improvements that might bring to my work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Strings\n",
    "\n",
    "In order for the computer to work with this reflection, we need use code or computer lanuage (in this case, Python) to manipulate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_GujJQgg8Qn",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Assign text (in the form of a muliline string) to a variable 'reflection'\n",
    "\n",
    "reflection = \"\"\"\n",
    "Yesterday I went to a seminar on deep learning that really challenged me to consider whether I am taking full advantage of this new technology in my research. Jeremy and Rachel presented about why deep learning is not over-hyped, and how it is going to be significant in the longer term. Initially, I was underwhelmed by this idea, and was thinking that the seminar might not hold much value for me. However, some of the material they presented was surprising and challenged me to re-think where I was at. Jeremy suggested that deep learning technologies were similar to electricity and that often the immediate applications are not obvious, but as time goes on, the technology is seen to be more valuable in many areas of society. He suggested that this could also be the case for deep learning technologies. I certainly hadn’t thought much about this with respect to my own research. However, this idea suggested that perhaps I need to reconsider whether it may be a valuable tech to add to my current technologies that I use.\n",
    " I have tended to avoid using the technology in my own research, for many reasons, and Jeremy and Rachel highlighted some of these reasons as common reasons why people avoid the technology. They then presented counter examples to challenge that thinking. This certainly challenged me in some ways, and I at times I felt a bit uncomfortable in seeing how flimsy some of my reasoning was. \n",
    "Certainly, moving forward I should probably be more open to how some of these new developments might be able to aid the advancement of my work in reflective writing analytics. I think that although there is still good reason to be careful, and aware of poor uses of this technology, I have tended to use this as bit of an excuse to avoid taking the extra effort to try and apply the technology in my own work.\n",
    "Moving forward, I need to take some time to look carefully at which applications might advance my research, and give them a go. Although I need to balance the use of deep learning with my other techniques, I should be careful not to favour those techniques that have served me well in the past, and be blinded to the potential of new (and potentially better) techniques. I should at least be open to the improvements that might bring to my work.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt8UK74sjgqn",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The variable `reflection` contains one long **string** of characters (each character is a byte stored in the computers memory)\n",
    "Notice that it doesn't include any of the normal formatting that we associate with text. It doesn't even know words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RCAlld4hOuw",
    "outputId": "aad35895-6daa-4c68-f939-800025f63a16",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Rendering\n",
    "\n",
    "To make it more readable, we can display the text as `HTML` (the structure of data in web browsers) - the browser will parse the HTML in a way that makes it easier to see the whole text. This is basic text visualisation in the browser. The browser is doing some basic work in translating raw strings into formats that humans are used to reading. We're not doing any analysis yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "9s5E5xBWh28_",
    "outputId": "c65a2567-be5b-439c-96e4-01b766149768",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import display.HTML and use it display the text as HTML\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "display.HTML(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lU5LtTUkRJd",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### More human requires more code!\n",
    "\n",
    "We could make it more readable still by turning the text into a list of paragraphs and displaying each with a space between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "3TrJN19UkrF-",
    "outputId": "951b9895-49e7-4954-9c7c-b1fb3e520cdb",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create a list of paragraphs\n",
    "\n",
    "reflection_paras = reflection.split('\\n')\n",
    "\n",
    "# wrap each paragraph in html <p> tags and separate with a </br> tag\n",
    "\n",
    "html_reflection = '</br>'.join(map(lambda x: '<p>'+x+'</p>', reflection_paras))\n",
    "display.HTML(html_reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UI_3GXkzn5U9",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Analysing with Natural Language Processing (NLP)\n",
    "\n",
    "To perform analysis on text, we generally make use of NLP libraries. Two of the most common libraries for the Python language are `Spacy` and `NLTK`. We will use Spacy ([spacy.io](http://spacy.io)) for our analyis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We may need to install the spacy 'library' or 'package' first before we can use it\n",
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LA1TsIq3dFCI",
    "outputId": "5b8e0f5f-6bc9-4fd2-a4e0-bad7f5fd935d",
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the spacy library and a pre-trained language model\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy.info() # Check version info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR2HwGvtqURV",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We're now ready to process our text with Spacy. For this exercise, we'll just do a simple analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EXpGBiDPfNhx",
    "outputId": "36ef5617-9d87-4249-f94a-e38e4f4fb75d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# process the reflection as a spacy Doc object\n",
    "\n",
    "doc = nlp(reflection)\n",
    "\n",
    "# view some of the information stored in the Doc object\n",
    "\n",
    "for token in doc[:10]:\n",
    "  print(token.idx, token.text, token.lemma_, token.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7u4t59YrnIG",
    "outputId": "3d676605-e946-4686-fd92-7c8972ce7b2f",
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Output the doc in json format to get an overview of all of the features\n",
    "\n",
    "doc.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Meaningful features\n",
    "\n",
    "Some of the NLP features look meaningful straight away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2x90qYU6fg-P",
    "outputId": "b1fd4c02-7db7-4f19-de41-0fe14328e5c0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# spacy also identified named entities\n",
    "\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "uMAXSCRuXneK",
    "outputId": "d88c35ad-a7de-4112-c789-7055a680b207",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# these can be visualised\n",
    "\n",
    "from spacy import displacy\n",
    "ent_render = displacy.render(doc, style=\"ent\")\n",
    "display.HTML(ent_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Useful features\n",
    "\n",
    "Other features don't appear meaningful, but perhaps they could be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "K4ndcapjflae",
    "outputId": "d6ff6d3e-c58c-4818-8c80-a45139402691",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can also visualise the dependency tree\n",
    "\n",
    "sentences = [sent for sent in doc.sents]\n",
    "deptree = displacy.render(sentences[0], style='dep')\n",
    "display.HTML(deptree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 3: Using NLP (towards WA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QnDZd-_JQYB",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we decide what might be useful out of this low level analysis, and how can we use it to construct features that are more aligned with how humans think about the writing?\n",
    "\n",
    "One approach is to think about meaningful patterns in the text and use the NLP software to extract them..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pattern matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFgYsyxUJkg5",
    "outputId": "85cc140e-3e7a-4a98-9b25-997a5cc2bc42",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Match a pattern in the text\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Find a modal verb followed by any word\n",
    "pattern = [{\"TAG\": \"PRP\"},{\"TAG\": \"MD\"},{\"POS\": \"ADV\", \"OP\": \"*\"},{},{}]\n",
    "matcher.add(\"example\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(start, end, span.text)\n",
    "\n",
    "matcher.remove(\"example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hsyetET1QL62",
    "outputId": "cd67c0a4-648b-49b0-e255-c2968d471b5f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can turn this into a function to allow us to experiment\n",
    "\n",
    "def findPattern(match_pattern):\n",
    "  matcher.add(\"experiment\", [match_pattern])\n",
    "  matches = matcher(doc)\n",
    "  matcher.remove(\"experiment\")\n",
    "  return [doc[start:end].text for match_id, start,end in matches]\n",
    "\n",
    "# Try it with pattern above\n",
    "\n",
    "findPattern(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hYFhsFKxHG1",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Finding patterns that are useful\n",
    "\n",
    "Experiment with matching patterns using the [Rule Based Matcher Explorer](https://explosion.ai/demos/matcher) and then try using the resulting pattern in the `findPattern` function below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjM1SlUxQ-Sg",
    "outputId": "2423142a-fe95-4e80-8f27-c2111686767c",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try your pattern out here\n",
    "\n",
    "my_pattern = [{'POS': 'PRON', 'OP': '?'},\n",
    "           {'POS': 'ADV', 'OP': '?'},\n",
    "           {'POS': 'VERB', 'OP': '?'},\n",
    "           {'TAG': 'PRP', 'OP': '?'}]\n",
    "\n",
    "findPattern(my_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1C1njsjYHVN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 4: Reflexive Expressions (WIP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "NLP2Features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
